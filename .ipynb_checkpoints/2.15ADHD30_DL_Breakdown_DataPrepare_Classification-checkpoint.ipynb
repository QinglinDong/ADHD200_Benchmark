{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Group analysis of resting-state fMRI with ICA: CanICA\n",
    "=====================================================\n",
    "\n",
    "An example applying CanICA to resting-state data. This example applies it\n",
    "to 30 subjects of the ADHD200 datasets. Then it plots a map with all the\n",
    "components together and an axial cut for each of the components separately.\n",
    "\n",
    "CanICA is an ICA method for group-level analysis of fMRI data. Compared\n",
    "to other strategies, it brings a well-controlled group model, as well as a\n",
    "thresholding algorithm controlling for specificity and sensitivity with\n",
    "an explicit model of the signal. The reference papers are:\n",
    "\n",
    "    * G. Varoquaux et al. \"A group model for stable multi-subject ICA on\n",
    "      fMRI datasets\", NeuroImage Vol 51 (2010), p. 288-299\n",
    "\n",
    "    * G. Varoquaux et al. \"ICA-based sparse features recovery from fMRI\n",
    "      datasets\", IEEE ISBI 2010, p. 1177\n",
    "\n",
    "Pre-prints for both papers are available on hal\n",
    "(http://hal.archives-ouvertes.fr)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from decomposition\n",
    "    estimators is implemented from version 0.4.1.\n",
    "    For older versions, unmask the deprecated attribute `components_`\n",
    "    to get the components image using attribute `masker_` embedded in\n",
    "    estimator.\n",
    "    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the ADHD200 data\n",
    "-------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First functional nifti image (4D) is at: /home/share/TmpData/Qinglin/nilearn_data/adhd/data/0010042/0010042_rest_tshift_RPI_voreg_mni.nii.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nilearn/datasets/func.py:503: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype=None)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "adhd_dataset = datasets.fetch_adhd(n_subjects=30,data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "func_filenames = adhd_dataset.func  # list of 4D nifti files for each subject\n",
    "\n",
    "# print basic information on the dataset\n",
    "print('First functional nifti image (4D) is at: %s' %\n",
    "      func_filenames[0])  # 4D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uga_qinglin/.local/lib/python3.5/site-packages/nibabel/nifti1.py:582: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  ext_def = np.fromstring(ext_def, dtype=np.int32)\n"
     ]
    }
   ],
   "source": [
    "from nilearn._utils.niimg_conversions import _resolve_globbing\n",
    "imgs = _resolve_globbing(func_filenames)\n",
    "\n",
    "from nilearn.masking import compute_epi_mask\n",
    "mask_img = compute_epi_mask(func_filenames)\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=mask_img, \n",
    "                     standardize=True,\n",
    "                     detrend=1,\n",
    "                     smoothing_fwhm=6.,\n",
    "                     memory=\"/storage/nilearn_cache\", \n",
    "                     memory_level=2)\n",
    "fmri_masked = masker.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decomposition.base import mask_and_reduce\n",
    "\n",
    "data = mask_and_reduce(\n",
    "    masker, imgs, confounds=None,\n",
    "    reduction_ratio=0.1,\n",
    "    random_state=0,\n",
    "    memory_level=3,\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply CanICA on the data\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decomposition import DictLearning\n",
    "\n",
    "dict_learning = DictLearning(n_components=40, memory_level=2,\n",
    "                             verbose=1,\n",
    "                             random_state=0,\n",
    "                             n_epochs=1)\n",
    "dict_learning.masker_=masker\n",
    "dict_learning._raw_fit(data)\n",
    "\n",
    "# Retrieve the independent components in brain space. Directly\n",
    "# accesible through attribute `components_img_`. Note that this\n",
    "# attribute is implemented from version 0.4.1. For older versions,\n",
    "# see note section above for details.\n",
    "components_img = dict_learning.components_img_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize we plot the outline of all components on one figure\n",
    "-----------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_prob_atlas\n",
    "\n",
    "# Plot all ICA components together\n",
    "plot_prob_atlas(components_img, title='All ICA components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the map for each ICA component separately\n",
    "-----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "\n",
    "for i, cur_img in enumerate(iter_img(components_img)):\n",
    "    plot_stat_map(cur_img, display_mode=\"z\", title=\"IC %d\" % i,\n",
    "                  cut_coords=1, colorbar=False)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Region Extractor algorithm from regions module\n",
    "# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n",
    "# maps, less the threshold means that more intense non-voxels will be survived.\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            standardize=True, min_region_size=5000)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "from nilearn.plotting import plot_prob_atlas\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %s components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 'all'))\n",
    "plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n",
    "                         title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "confounds = adhd_dataset.confounds\n",
    "phenotypic = adhd_dataset.phenotypic\n",
    "adhd_time_series = []\n",
    "all_time_series = []\n",
    "site_names = []\n",
    "adhd_labels = []  # 1 if ADHD, 0 if control\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound, phenotypic in zip(func_filenames, confounds,phenotypic):\n",
    "    time_series = extractor.transform(filename, confounds=confound)   \n",
    "    all_time_series.append(time_series)\n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_time_series.append(time_series)\n",
    "    site_names.append(phenotypic['site'])\n",
    "    adhd_labels.append(is_adhd)        \n",
    "print('Data has {0} ADHD subjects.'.format(len(adhd_time_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we need to do subjects timeseries signals extraction and then estimating\n",
    "# correlation matrices on those signals.\n",
    "# To extract timeseries signals, we call transform() from RegionExtractor object\n",
    "# onto each subject functional data stored in func_filenames.\n",
    "# To estimate correlation matrices we import connectome utilities from nilearn\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "confounds = adhd_dataset.confounds\n",
    "phenotypic = adhd_dataset.phenotypic\n",
    "adhd_time_series = []\n",
    "all_time_series = []\n",
    "site_names = []\n",
    "adhd_labels = []  # 1 if ADHD, 0 if control\n",
    "# Initializing ConnectivityMeasure object with kind='correlation'\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "for filename, confound, phenotypic in zip(func_filenames, confounds,phenotypic):\n",
    "    time_series = extractor.transform(filename, confounds=confound)   \n",
    "    all_time_series.append(time_series)\n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_time_series.append(time_series)\n",
    "    site_names.append(phenotypic['site'])\n",
    "    adhd_labels.append(is_adhd)        \n",
    "print('Data has {0} ADHD subjects.'.format(len(adhd_time_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_biomarkers = {}\n",
    "\n",
    "conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
    "connectivity_biomarkers = conn_measure.fit_transform(all_time_series)\n",
    "\n",
    "# For each kind, all individual coefficients are stacked in a unique 2D matrix.\n",
    "print('{0} correlation biomarkers for each subject.'.format(\n",
    "    connectivity_biomarkers.shape[1]))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "classes = ['{0}{1}'.format(site_name, adhd_label)\n",
    "           for site_name, adhd_label in zip(site_names, adhd_labels)]\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mean_scores = []\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv_scores = cross_val_score(clf,\n",
    "                                connectivity_biomarkers,\n",
    "                                y=adhd_labels,\n",
    "                                cv=cv,\n",
    "                                groups=adhd_labels,\n",
    "                                scoring='accuracy',\n",
    "                                )\n",
    "    mean_scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import show\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(6, 4))\n",
    "positions = np.arange(len(names)) * .1 + .1\n",
    "plt.barh(positions, mean_scores, align='center', height=.05)\n",
    "yticks = [name.replace(' ', '\\n') for name in names]\n",
    "plt.yticks(positions, yticks)\n",
    "plt.xlabel('Classification accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
