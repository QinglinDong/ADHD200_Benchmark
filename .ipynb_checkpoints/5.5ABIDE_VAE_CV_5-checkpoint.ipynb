{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "def build_model(data):\n",
    "    batch_size = 20\n",
    "    original_dim=data.shape[1]\n",
    "    latent_dim = 64\n",
    "\n",
    "    epochs = 300\n",
    "    epsilon_std = 1.0\n",
    "\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(2048, activation='tanh',\n",
    "                    activity_regularizer=regularizers.l1(1*10e-5))(x)\n",
    "    h = Dense(1024, activation='tanh',\n",
    "                    activity_regularizer=regularizers.l1(1*10e-5))(h)\n",
    "    h = Dense(512, activation='tanh',\n",
    "                    activity_regularizer=regularizers.l1(1*10e-5))(h)\n",
    "    h = Dense(256, activation='tanh',\n",
    "                    activity_regularizer=regularizers.l1(1*10e-5))(h)\n",
    "    h = Dense(128, activation='tanh',\n",
    "                    activity_regularizer=regularizers.l1(1*10e-5))(h)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                                  stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "\n",
    "    h_decoded = Dense(128, activation='tanh')(z)\n",
    "    h_decoded = Dense(256, activation='tanh')(h_decoded)\n",
    "    h_decoded = Dense(512, activation='tanh')(h_decoded)\n",
    "    h_decoded = Dense(1024, activation='tanh')(h_decoded)\n",
    "    h_decoded = Dense(2048, activation='tanh')(h_decoded)\n",
    "    x_decoded_mean = Dense(original_dim, activation='tanh')(h_decoded)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Compute VAE loss\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x,x_decoded_mean )\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    from keras import optimizers\n",
    "    rmsprop=optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    vae.compile(optimizer=rmsprop)\n",
    "    vae.summary()\n",
    "\n",
    "\n",
    "    vae.fit(data,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    encoder = Model(x, z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decomposition import CanICA\n",
    "def prepare_data(func_filenames):\n",
    "    canica = CanICA(memory=\"nilearn_cache\", memory_level=2,\n",
    "                    threshold=3., verbose=10, random_state=0, \n",
    "                    mask='/home/share/TmpData/Qinglin/ADHD200_Athena_preproc_flirtfix/ADHD200_mask_152_4mm.nii.gz')\n",
    "    data=canica.prepare_data(func_filenames)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "def corr(all_time_series):\n",
    "    connectivity_biomarkers = {}\n",
    "    conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True)\n",
    "    connectivity_biomarkers = conn_measure.fit_transform(all_time_series)\n",
    "    return connectivity_biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "def corr_tan(all_time_series):\n",
    "    connectivity_biomarkers = {}\n",
    "    tangent_measure = ConnectivityMeasure(kind='tangent', vectorize=True)\n",
    "    connectivity_biomarkers = tangent_measure.fit_transform(all_time_series)\n",
    "    return connectivity_biomarkers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.load('/home/share/TmpData/Qinglin/ABIDE/X.npy')\n",
    "Y=np.load('/home/share/TmpData/Qinglin/ABIDE/Y.npy')\n",
    "model=build_model(X)\n",
    "\n",
    "from nilearn.datasets import fetch_abide_pcp\n",
    "\n",
    "# We specify the site and number of subjects we want to download\n",
    "abide = fetch_abide_pcp(derivatives=['func_preproc'], data_dir='/home/share/TmpData/Qinglin/nilearn_data/')\n",
    "\n",
    "# We look at the available data in this dataset\n",
    "print(abide.keys())\n",
    "\n",
    "\n",
    "func_filenames = abide.func_preproc  # list of 4D nifti files for each subject\n",
    "\n",
    "from nilearn._utils.niimg_conversions import _resolve_globbing\n",
    "imgs = _resolve_globbing(func_filenames)\n",
    "\n",
    "mask_img ='/home/share/TmpData/Qinglin/ADHD200_Athena_preproc_flirtfix/ADHD200_mask_152_4mm.nii.gz'\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=mask_img, \n",
    "                     standardize=True,\n",
    "                     detrend=1,\n",
    "                     smoothing_fwhm=6.,\n",
    "                     memory=\"/storage/nilearn_cache\", \n",
    "                     memory_level=2)\n",
    "fmri_masked = masker.fit()\n",
    "\n",
    "from nilearn.decomposition.base import _mask_and_reduce_single\n",
    "all_time_series=[]\n",
    "for img in imgs:\n",
    "    print(img)\n",
    "    data = _mask_and_reduce_single(\n",
    "        masker, img, confound=None,\n",
    "        reduction_ratio=1,\n",
    "        random_state=0,\n",
    "        memory_level=3)\n",
    "    time_series=encoder.predict(data,\n",
    "        batch_size=batch_size)\n",
    "    all_time_series.append(time_series)\n",
    "\n",
    "D=all_time_series    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\",\"XGBoost\",\"Bagging\",\"GTBoosting\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "#         SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#         GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=100, max_features=1),\n",
    "    MLPClassifier(hidden_layer_sizes=(500,400,300,200,100,50)),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    XGBClassifier(),\n",
    "    BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)]\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "mean_scores = []\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    cv_scores = cross_val_score(clf,\n",
    "                                corr_tan(D),\n",
    "                                y=Y,\n",
    "                                cv=cv,\n",
    "                                groups=Y,\n",
    "                                scoring='accuracy',\n",
    "                                )\n",
    "    mean_scores.append(cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import show\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "positions = np.arange(len(names)) * .1 + .1\n",
    "plt.barh(positions, mean_scores, align='center', height=.05)\n",
    "yticks = [name.replace(' ', '\\n') for name in names]\n",
    "plt.yticks(positions, yticks)\n",
    "plt.xlabel('Classification accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
